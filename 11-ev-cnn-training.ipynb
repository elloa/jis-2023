{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "745262ac-e3d5-4dc4-84cb-b9112c7a4286",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ShuffleNet Training - Extended Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8c42e3-935d-4521-b4f9-69f68167166a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import time, os, shutil\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, plot_confusion_matrix\n",
    "from tensorflow.keras import layers, models, optimizers, callbacks\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Add, AveragePooling2D, Dense, AvgPool2D,BatchNormalization, ReLU, DepthwiseConv2D, Reshape, Permute,Conv2D, MaxPool2D, GlobalAveragePooling2D, concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5b25f1-194f-4d79-a35d-bcbb0ad6bbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690cbcae-edb8-4712-b02c-813b524a6c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "tf.get_logger().setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6762632a-fbcc-42ae-a770-966d44220f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.debugging.set_log_device_placement(False)\n",
    "devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(devices[0], True)\n",
    "tf.config.experimental.set_memory_growth(devices[1], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8d0ed7-99d4-4b63-8c4b-8149de80a189",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_FOLDER = \"/home/lais/jis-2023\"\n",
    "PATH_DATASETS = ROOT_FOLDER + \"/datasets\"\n",
    "\n",
    "PATH_BRACOL = PATH_DATASETS + \"/bracol\"\n",
    "PATH_BRACOL_SPLITTED = PATH_BRACOL + \"/resized_splitted\"\n",
    "\n",
    "PATH_PLANT_PATOLOGIES = PATH_DATASETS + \"/plant_patologies\"\n",
    "PATH_PLANT_PATOLOGIES_SPLITTED = PATH_PLANT_PATOLOGIES + \"/splitted\"\n",
    "\n",
    "PATH_ROCOLE = PATH_DATASETS + \"/rocole\"\n",
    "PATH_ROCOLE_SPLITTED = PATH_ROCOLE + \"/splitted\"\n",
    "\n",
    "PATH_EV_ARTIFACTS = ROOT_FOLDER + \"/cnn-artifacts/extended-validation-artifacts\"\n",
    "PATH_EVALUATIONS = ROOT_FOLDER + \"/evaluations\"\n",
    "PATH_EVALUATIONS_EV = PATH_EVALUATIONS + \"/extended-validation\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d336168-8bed-4352-963e-c51c70340448",
   "metadata": {},
   "source": [
    "## 1. Defining hiperparams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20eae564-4ea2-46e3-9b65-ac8c0e81edb9",
   "metadata": {},
   "source": [
    "### 1.1. Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b7ec49-bd6c-42b4-99e4-e2d1caa7914a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "IMG_SIZE = (128, 128)\n",
    "NUM_LABELS_BRACOL = 5\n",
    "NUM_LABELS_PLANT_PATOLOGIES = 2\n",
    "NUM_LABELS_ROCOLE = 3\n",
    "INPUT_SHAPE = (128, 128, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db31b5e1-ded3-4734-87e1-53cb579c99df",
   "metadata": {},
   "source": [
    "### 1.2. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06870921-fc80-4af1-ad0c-147e0eb2a861",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 300\n",
    "patience = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805e43c7-56e1-4a4d-b4cb-5bc41d5c88a4",
   "metadata": {},
   "source": [
    "## 2. Defining model evaluation functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e16901-e81e-4544-915e-77c2ab18ab1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graph(epochs, train_data, train_label, val_data, val_label, title, xlabel, ylabel, path, metric, early_stopping):\n",
    "    \"\"\"Plot line graph from model data.\"\"\"\n",
    "    \n",
    "    plt.plot(epochs, train_data, label=train_label, color=\"darkblue\")\n",
    "    plt.plot(epochs, val_data, label=val_label, color=\"cornflowerblue\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.legend()\n",
    "    \n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    \n",
    "    if early_stopping:\n",
    "        plt.savefig(f\"{path}/{metric}-ES.jpg\", dpi=115, bbox_inches=\"tight\")\n",
    "    else:\n",
    "        plt.savefig(f\"{path}/{metric}.jpg\", dpi=115, bbox_inches=\"tight\")\n",
    "        \n",
    "    plt.close()\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7518a18e-5cb7-4a1c-893f-5ac768615b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_training_progress(history, path, early_stopping=True):\n",
    "    \"\"\"Generate visual representation from model training progress and return total training epochs.\"\"\"\n",
    "    \n",
    "    acc = history.history[\"acc\"]\n",
    "    val_acc = history.history[\"val_acc\"]\n",
    "\n",
    "    loss = history.history[\"loss\"]\n",
    "    val_loss = history.history[\"val_loss\"]\n",
    "\n",
    "    epochs = range(1, len(acc) + 1)\n",
    "    \n",
    "    plot_graph(epochs, acc, \"Train\", val_acc, \"Validation\", \"Train and Validation Accuracy\", \"Epochs\", \"Accuracy\", path, \"Accuracy\", early_stopping)\n",
    "    plot_graph(epochs, loss, \"Train\", val_loss, \"Validation\", \"Train and Validation Loss\", \"Epochs\", \"Loss\", path, \"Loss\", early_stopping)\n",
    "    \n",
    "    return len(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e4d14c-ad9d-4aea-876f-1be956de7541",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_true, y_pred):\n",
    "    \"\"\"Calculate model metrics.\"\"\"\n",
    "    \n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, average=\"weighted\")\n",
    "    precision = precision_score(y_true, y_pred, average=\"weighted\")\n",
    "    recall = recall_score(y_true, y_pred, average=\"weighted\")\n",
    "    \n",
    "    return accuracy, f1, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646a15cd-677f-4d09-a72e-b5fe850c7a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataframe(early_stopping, epochs, accuracy, f1, precision, recall, path, dataset):\n",
    "    \"\"\"Generate dataframe to save model metrics.\"\"\"\n",
    "    \n",
    "    metrics = {\n",
    "        \"Early Stopping\": early_stopping, \n",
    "        \"Epochs\": epochs, \n",
    "        \"Test Accuracy\": accuracy, \n",
    "        \"Test F1 Weightet\": f1, \n",
    "        \"Test Precision Weighted\": precision, \n",
    "        \"Test Recall Weighted\": recall}\n",
    "    \n",
    "    df_new = pd.DataFrame(data=[metrics])\n",
    "    file_path = f\"{path}/metrics_{dataset}.csv\"\n",
    "    \n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        df = pd.read_csv(file_path)\n",
    "        df = df.append(df_new)\n",
    "        df.to_csv(file_path, header=True, index=False)\n",
    "    else:\n",
    "        df_new.to_csv(file_path, header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb4131d-3cec-4ec5-9152-7743e69515bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix_scorer(model, class_names, test_generator, y_true_test, path, early_stopping):\n",
    "    \"\"\"Generate confusion matrix from model predictions.\"\"\"\n",
    "    \n",
    "    class estimator:\n",
    "        _estimator_type = ''\n",
    "        classes_= []\n",
    "\n",
    "        def __init__(self, model, classes):\n",
    "            self.model = model\n",
    "            self._estimator_type = \"classifier\"\n",
    "            self.classes_ = classes\n",
    "\n",
    "        def predict(self, X):\n",
    "            y_prob= self.model.predict(X)\n",
    "            y_pred = y_prob.argmax(axis=1)\n",
    "            return y_pred\n",
    "\n",
    "\n",
    "    classifier = estimator(model, class_names)\n",
    "    \n",
    "    cm = plot_confusion_matrix(estimator=classifier, X=test_generator, y_true=y_true_test, xticks_rotation=45, cmap=\"Blues\")\n",
    "    cm.ax_.set_title(f\"Confusion Matrix\")\n",
    "    cm.ax_.set_xlabel(\"Predicted labels\")\n",
    "    cm.ax_.set_xticklabels(class_names)\n",
    "    cm.ax_.set_ylabel(\"True labels\")\n",
    "    cm.ax_.set_yticklabels(class_names)\n",
    "    \n",
    "    file_path = f\"{path}/ConfusionMatrix-ES.jpg\"\n",
    "    \n",
    "    if not early_stopping:\n",
    "        file_path = f\"{path}/ConfusionMatrix.jpg\"\n",
    "    \n",
    "    plt.savefig(file_path, dpi=115, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39cec1ae-bd54-48dc-bb5b-18e20b45ab9a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Defining image generator function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1d7e31-960b-4f0f-b9d0-02a6fce8e58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_data_generator(train_dir, validation_dir, test_dir):\n",
    "    \"\"\"Construct image data generator.\"\"\"\n",
    "    \n",
    "    train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "    validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "    test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=IMG_SIZE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode=\"categorical\")\n",
    "\n",
    "    validation_generator = validation_datagen.flow_from_directory(\n",
    "        validation_dir,\n",
    "        target_size=IMG_SIZE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode=\"categorical\")\n",
    "\n",
    "    test_generator = test_datagen.flow_from_directory(\n",
    "        test_dir,\n",
    "        target_size=IMG_SIZE,\n",
    "        batch_size=1,\n",
    "        class_mode=\"categorical\",\n",
    "        shuffle = False)\n",
    "    \n",
    "    return train_generator, validation_generator, test_generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6420bc51-5dc3-43a4-8edd-4cf70ff3593b",
   "metadata": {},
   "source": [
    "## 4. Training ShuffleNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe139f16-279c-41e3-8caf-fd3c75558588",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4.1. Defining functions to return model to train with adjustments to output layers to match the problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed91518-59f4-4002-b454-a3c8754ce9cb",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Defining architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c3899a-136d-4155-9d11-7f26fd9a0189",
   "metadata": {},
   "outputs": [],
   "source": [
    "def channel_shuffle(x, groups):  \n",
    "    _, width, height, channels = x.get_shape().as_list()\n",
    "    group_ch = channels // groups\n",
    "\n",
    "    x = Reshape([width, height, group_ch, groups])(x)\n",
    "    x = Permute([1, 2, 4, 3])(x)\n",
    "    x = Reshape([width, height, channels])(x)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4dd6c1-9750-4369-a8b7-a0ff81488cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_unit(x, groups, channels,strides):\n",
    "    y = x\n",
    "    x = Conv2D(channels//4, kernel_size = 1, strides = (1,1),padding = 'same', groups=groups)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "\n",
    "    x = channel_shuffle(x, groups)\n",
    "    \n",
    "    x = DepthwiseConv2D(kernel_size = (3,3), strides = strides, padding = 'same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    if strides == (2,2):\n",
    "        channels = channels - y.shape[-1]\n",
    "        \n",
    "    x = Conv2D(channels, kernel_size = 1, strides = (1,1),padding = 'same', groups=groups)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    if strides ==(1,1):\n",
    "        x =Add()([x,y])\n",
    "        \n",
    "    if strides == (2,2):\n",
    "        y = AvgPool2D((3,3), strides = (2,2), padding = 'same')(y)\n",
    "        x = concatenate([x,y])\n",
    "    \n",
    "    x = ReLU()(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b243854-78c3-44d9-9dd6-11b03c953b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ShuffleNet(n_classes, start_channels, input_shape=(224, 224, 3)):\n",
    "    groups = 2\n",
    "    input = Input(input_shape)\n",
    "\n",
    "    x =  Conv2D (24,kernel_size=3,strides = (2,2), padding = 'same', use_bias = True)(input)\n",
    "    x =  BatchNormalization()(x)\n",
    "    x =  ReLU()(x)\n",
    "    \n",
    "    x = MaxPool2D (pool_size=(3,3), strides = 2, padding='same')(x)\n",
    "\n",
    "    repetitions = [3,7,3]\n",
    "\n",
    "    for i,repetition in enumerate(repetitions):\n",
    "        channels = start_channels * (2**i)\n",
    "\n",
    "        x  = shuffle_unit(x, groups, channels,strides = (2,2))\n",
    "\n",
    "        for i in range(repetition):\n",
    "            x = shuffle_unit(x, groups, channels,strides=(1,1))\n",
    "\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "    output = Dense(n_classes,activation='softmax')(x)\n",
    "\n",
    "    model = Model(input, output)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e004ffa5-68d0-4ef9-9331-b12928b35955",
   "metadata": {
    "tags": []
   },
   "source": [
    "- BRACOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c521872-20c8-44a5-ac50-a163fabe7477",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sn_bracol():\n",
    "    shufflenet = ShuffleNet(NUM_LABELS_BRACOL, 200, INPUT_SHAPE)\n",
    "\n",
    "    last = shufflenet.layers[-2]\n",
    "    x = layers.Flatten()(last.output)\n",
    "    x = layers.Dense(512, activation=\"relu\")(x)\n",
    "    predictions = layers.Dense(NUM_LABELS_BRACOL, activation=\"sigmoid\")(x)\n",
    "    \n",
    "    shufflenet_architecture = models.Model(shufflenet.input, predictions)\n",
    "    return shufflenet_architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42996004-cca0-4ddf-ae97-d30a4314ea4d",
   "metadata": {
    "tags": []
   },
   "source": [
    "- Plant Patologies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29157bce-fcc9-4b42-a00b-8ea244196aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sn_plant_patologies():\n",
    "    shufflenet = ShuffleNet(NUM_LABELS_PLANT_PATOLOGIES, 200, INPUT_SHAPE)\n",
    "\n",
    "    last = shufflenet.layers[-2]\n",
    "    x = layers.Flatten()(last.output)\n",
    "    x = layers.Dense(512, activation=\"relu\")(x)\n",
    "    predictions = layers.Dense(NUM_LABELS_PLANT_PATOLOGIES, activation=\"sigmoid\")(x)\n",
    "    \n",
    "    shufflenet_architecture = models.Model(shufflenet.input, predictions)\n",
    "    return shufflenet_architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a60a1d-8e2b-4cc1-b26b-8ce58137d641",
   "metadata": {
    "tags": []
   },
   "source": [
    "- RoCoLe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd49ab9-7311-4e17-aa54-058de1a82934",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sn_rocole():\n",
    "    shufflenet = ShuffleNet(NUM_LABELS_ROCOLE, 200, INPUT_SHAPE)\n",
    "\n",
    "    last = shufflenet.layers[-2]\n",
    "    x = layers.Flatten()(last.output)\n",
    "    x = layers.Dense(512, activation=\"relu\")(x)\n",
    "    predictions = layers.Dense(NUM_LABELS_ROCOLE, activation=\"sigmoid\")(x)\n",
    "    \n",
    "    shufflenet_architecture = models.Model(shufflenet.input, predictions)\n",
    "    return shufflenet_architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cc56b4-9f3e-4a77-9ce2-922dc998f3de",
   "metadata": {},
   "source": [
    "#### Defining model to each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8650d4dc-403b-4e89-9f38-86335c1e6c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_datasets = {\n",
    "    \"bracol\": PATH_BRACOL_SPLITTED, \n",
    "    \"plant_patologies\": PATH_PLANT_PATOLOGIES_SPLITTED, \n",
    "    \"rocole\": PATH_ROCOLE_SPLITTED\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35a0aba-3c83-491d-8af9-d267e6919f56",
   "metadata": {},
   "source": [
    "### 4.2. Adjusting cnn-artifacts folder to save extended validation artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfa1d0f-d672-4c90-8726-16f54e6d959e",
   "metadata": {},
   "outputs": [],
   "source": [
    "jmuben_artifacts = ROOT_FOLDER + \"/cnn-artifacts/jmuben\"\n",
    "\n",
    "if not os.path.exists(jmuben_artifacts):\n",
    "    os.makedirs(jmuben_artifacts)\n",
    "\n",
    "for directory in os.listdir(ROOT_FOLDER + \"/cnn-artifacts\"):\n",
    "    if directory != \"jmuben\":\n",
    "        source = ROOT_FOLDER + f\"/cnn-artifacts/{directory}\"\n",
    "        destination = jmuben_artifacts + f\"/{directory}\"\n",
    "        shutil.move(source, destination)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34153610-4c65-4c3c-8e84-c8b29aa2ff9d",
   "metadata": {},
   "source": [
    "### 4.3. Training ShuffleNet in each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2236ab7b-aad6-4037-b2c0-0ce4a67628d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_early_stopping(model, train_generator, step_per_epoch, validation_generator, validation_steps, callbacks, artifacts_path, evaluations_path, checkpoints_path, dataset, early_stopping):\n",
    "    history = model.fit(\n",
    "        train_generator, \n",
    "        steps_per_epoch=step_per_epoch, \n",
    "        epochs=EPOCHS, \n",
    "        validation_data=validation_generator, \n",
    "        validation_steps=validation_steps, \n",
    "        callbacks=callbacks,\n",
    "        verbose=0)\n",
    "\n",
    "    # Saving the best model    \n",
    "    if early_stopping:\n",
    "        model.save(f\"{artifacts_path}/ShuffleNet_{dataset}_ES.h5\")\n",
    "    else:\n",
    "        model.save(f\"{artifacts_path}/ShuffleNet_{dataset}.h5\")\n",
    "    print(\"Best model saved.\")\n",
    "\n",
    "    # Model progress\n",
    "    model_evaluation_path = f\"{evaluations_path}/{dataset}\"\n",
    "    total_epochs = int\n",
    "    \n",
    "    if early_stopping:\n",
    "        total_epochs = model_training_progress(history, model_evaluation_path)\n",
    "    else:\n",
    "        total_epochs = model_training_progress(history, model_evaluation_path, False)\n",
    "        \n",
    "    print(\"Progress model saved.\")\n",
    "\n",
    "    # Loading model architecture and weights\n",
    "    model_json = model.to_json()\n",
    "    model = tf.keras.models.model_from_json(model_json)\n",
    "    \n",
    "    if early_stopping:\n",
    "        model.load_weights(f\"{checkpoints_path}/checkpoint_ES\")\n",
    "    else:\n",
    "        model.load_weights(f\"{checkpoints_path}/checkpoint\")\n",
    "\n",
    "    # Testing model\n",
    "    print(f\"\\nTesting ShuffleNet model...\")\n",
    "    STEP_SIZE_TEST = test_generator.n // test_generator.batch_size\n",
    "    test_generator.reset()\n",
    "\n",
    "    y_pred = model.predict(test_generator, steps=STEP_SIZE_TEST, verbose=1)\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "    # Mapping predicted integers to labels \n",
    "    labels = (train_generator.class_indices)\n",
    "    labels = dict((value, key) for key, value in labels.items())\n",
    "\n",
    "    y_pred = [labels[element] for element in y_pred]\n",
    "\n",
    "    y_true = test_generator.filenames\n",
    "    y_true = [element[:element.find('/')] for element in y_true]\n",
    "\n",
    "    # Evaluating model and saving metrics\n",
    "    accuracy, f1, precision, recall = evaluate_model(y_true, y_pred)\n",
    "    generate_dataframe(early_stopping, total_epochs, accuracy, f1, precision, recall, model_evaluation_path, dataset)\n",
    "    print(\"Model evaluated.\")\n",
    "\n",
    "    # Plotting confusion matrix\n",
    "    class_names = os.listdir(test_dir)\n",
    "\n",
    "    true_labels = []\n",
    "\n",
    "    for filename in y_true:\n",
    "        for key, value in labels.items():\n",
    "            if filename == value:\n",
    "                true_labels.append(key)\n",
    "\n",
    "    confusion_matrix_scorer(model, class_names, test_generator, true_labels, model_evaluation_path, early_stopping)\n",
    "    print(\"Confusion Matrix saved.\")\n",
    "    print(\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8a5f50-4a9a-4d66-8417-47b38437a5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in models_datasets:\n",
    "    path = models_datasets[dataset]\n",
    "    \n",
    "    # Directory\n",
    "    train_dir = f\"{path}/train\"\n",
    "    validation_dir = f\"{path}/val\"\n",
    "    test_dir = f\"{path}/test\"\n",
    "    \n",
    "    # Generators\n",
    "    train_generator, validation_generator, test_generator = image_data_generator(train_dir, validation_dir, test_dir)\n",
    "    \n",
    "    # Verifying if artifacts directory exists\n",
    "    ARTIFACTS_PATH = f\"{PATH_EV_ARTIFACTS}/{dataset}\"\n",
    "    checkpoints_path = f\"{ARTIFACTS_PATH}/checkpoints\"\n",
    "\n",
    "    if not os.path.isdir(ARTIFACTS_PATH):\n",
    "        os.makedirs(ARTIFACTS_PATH)\n",
    "    \n",
    "    if not os.path.isdir(checkpoints_path):\n",
    "        os.makedirs(checkpoints_path)\n",
    "    \n",
    "    # Callbacks - Early Stopping\n",
    "    csv_logger_es = callbacks.CSVLogger(f\"{ARTIFACTS_PATH}/training_ES.log\")\n",
    "    early_stopping = callbacks.EarlyStopping(monitor=\"val_acc\", patience=patience)\n",
    "    model_checkpoint_es = callbacks.ModelCheckpoint(\n",
    "        filepath=f\"{checkpoints_path}/checkpoint_ES\", \n",
    "        monitor=\"val_acc\", \n",
    "        save_best_only=True, \n",
    "        save_weights_only=True, \n",
    "        mode=\"max\")\n",
    "    \n",
    "    # Callbacks - No Early Stopping\n",
    "    csv_logger = callbacks.CSVLogger(f\"{ARTIFACTS_PATH}/training.log\")\n",
    "    model_checkpoint = callbacks.ModelCheckpoint(\n",
    "        filepath=f\"{checkpoints_path}/checkpoint\", \n",
    "        monitor=\"val_acc\", \n",
    "        save_best_only=True, \n",
    "        save_weights_only=True, \n",
    "        mode=\"max\")\n",
    "    \n",
    "    # Creating and compiling model\n",
    "    shuffle_net = sn_bracol()\n",
    "        \n",
    "    if \"plant\" in dataset:\n",
    "        shuffle_net = sn_plant_patologies()\n",
    "    elif \"rocole\" in dataset:\n",
    "        shuffle_net = sn_rocole()\n",
    "        \n",
    "    shuffle_net.compile(loss=\"categorical_crossentropy\", optimizer=optimizers.RMSprop(learning_rate=1e-4), metrics=[\"acc\"])\n",
    "    shuffle_net.summary()\n",
    "    \n",
    "    # Defining parameters based on dataset\n",
    "    step_per_epoch = len(train_generator)\n",
    "    validation_steps = len(validation_generator)\n",
    "    \n",
    "    # Train with Early Stopping callback\n",
    "    callbacks_es = [csv_logger_es, early_stopping, model_checkpoint_es]\n",
    "    train_test_early_stopping(shuffle_net, train_generator, step_per_epoch, validation_generator, validation_steps, callbacks_es, ARTIFACTS_PATH, PATH_EVALUATIONS_EV, checkpoints_path, dataset, True)\n",
    "    \n",
    "    # Train without Early Stopping callback\n",
    "    callbacks_no_es = [csv_logger, model_checkpoint]\n",
    "    train_test_early_stopping(shuffle_net, train_generator, step_per_epoch, validation_generator, validation_steps, callbacks_no_es, ARTIFACTS_PATH, PATH_EVALUATIONS_EV, checkpoints_path, dataset, False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
